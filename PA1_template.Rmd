# Reproducible Research (Peer Assessment 1)  
**Data Science -> Reproducible Research -> Peer Assessment 1**   
   
   
-------------------  

### Welcome  

Hi,  
Welcome to my first Course Project / Peer Assessment Assignment.  
The purpose of this work is to produce a R Markdown document with all the code needed to answer the required questions.  
  
This code was prepared in the following environment:
 - OS: Linux x86_64 (3.12.15-pclos1)
 - R/RStudio: 3.0.2 / 99.9.9, with the following packages:
   - tools (3.0.2)
   - knitr (1.6)
   - ggplot2 (1.0.0)
  
I'll start by preparing the environment to load required packages and to always display the code chunks in the html output file.  

```{r Setup}
library(tools)
library(knitr)
library(ggplot2)
opts_chunk$set(echo=TRUE, results="markup")
opts_knit$set(verbose=TRUE)

```
   
-------------------
### Loading and preprocessing the data

I will use two separate code chunks.  
  
The first code chunk is responsible to make sure the right data file is in the directory and then it loads it the `dataset` variable.  
First the code checks if the file is present (it should since it is in the repository, but I'll test it just in case). In case it is not present the script tries to download the data from the [course web site](https://d396qusza40orc.cloudfront.net/repdata/data/activity.zip) (and raises an error in case the download did not complete successfully).  
If the file is present I'll make sure it is the right file by comparing its MD5 hash with a previously computed one. MD5 isn't cryptographically secure anymore, but it's enough for my checksumming needs. You can find more information about MD5 [here](http://en.wikipedia.org/wiki/MD5). If the hashes do not match the code will not continue.  
If something goes wrong during execution an error should be raised. In case no handled error occurs the data is loaded from inside the zip file and message is returned to let user know everything is fine.

```{r Loading}
kRemote <- "https://d396qusza40orc.cloudfront.net/repdata/data/activity.zip"
kFile   <- "activity.zip"
kFileDt <- "activity.csv"
kFileMd <- "activity.md5"
    
if (!file.exists(kFile)){
    # File is not present at the working directory. Let's download it!
    method <-"auto"  # Default method: shall be fine for MS Windows (untested!)
    if (.Platform$OS.type=="unix") method <- "curl"  # Use for unix-like systems
    download.file(kRemote, kFile, method, FALSE, "wb")
}
# stop() doesn't work inside knitr. Aarrghh! :(
# I had to change the way the test is preformed. I can actualy use print()
if (!file.exists(kFile)){
    stop("Download failed!")
} else {
    # File is now present (it was before or it was sucessfully downloaded).
    # Let's check if it is the expected file. I'll compare it's MD5 hash with
    # a precomputed one.
    if (readLines(kFileMd)!= as.vector(md5sum(kFile))){
        stop("File is not correct!")
    } else {
        # The correct file shall be present. I'll read the data directly from 
        # inside the Zip file.
        myConn <- unz(kFile, kFileDt)
        dataset <- read.csv(myConn, na.strings="NA", stringsAsFactors=FALSE)
        print("Data sucessfully loaded!")
    }
}

```
  
    
A second phase is the data preprocessing to make it suitable for further analysis.  
To start with, I will add 4-length zeros to the beginning of the interval data. This will result in a 24h time format 'hhmm'.


``` {r Preprocessing}
if (!exists("dataset")) {
    stop("It looks something went wrong in the 'Loading' chunk!")
} else {
    # Processing Time information by adding x times a 0 (zero) before the actual
    # data.
    f <- dataset$interval
    f <- ifelse(nchar(f)==3, paste0("0", f), f)
    f <- ifelse(nchar(f)==2, paste0("00", f), f)
    f <- ifelse(nchar(f)==1, paste0("000", f), f)
    dataset$time <- as.factor(f)
    dataset <- dataset[, -3] #remove interval
    #Transform date field from chr to date+time.
    dataset$date <- as.POSIXct(paste(dataset$date,f), format="%Y-%m-%d %H%M")
    #Finishing
    print("Preprocessing finished!")
}

```
-------------------

### What is mean total number of steps taken per day?

After loading and cleaing the data a little bit, we need to do the following:  
1. Make a histogram of the total number of steps taken each day;  
2. Calculate and report the *mean* and *median* total number of steps taken per day.  
  
To do this I'll start by grouping the original dataset by each day. since I'm dependent on the data loaded and computed before I'll check it actually exists.

```{r Histogram, dev='png', fig.width=10, fig.height=5}
if (!exists("dataset")) {
    stop("It looks something went wrong in the 'Loading' or 'Preprocessing' chunk!")
} else {
    # sum the number of steps by date
    group <- aggregate(steps~as.Date(date), dataset, sum)
    names(group) <- c("Date", "Steps")
    
    hist <- ggplot(group, aes(Date, Steps))
    hist <- hist + geom_histogram(stat="identity", col="gold1", fill="royalblue")
    hist <- hist + labs(title="Number of Steps by Date", x="Date", y="# Steps")
    hist <- hist + theme_classic(base_size=14)
    
    #Outputting the histogram
    print(hist)
}

```

Now it's time to compute the *mean* and *median*.  
I'll reuse the grouped data computed for the Histogram, so I start by checking the `group` variable exists.  


```{r M&M}
if (!exists("group")) {
    stop("It looks something went wrong in the 'Histogram' chunk!")
} else {
    cat("Mean:", mean(group$Steps), "\n")
    cat("Median:", median(group$Steps), "\n")
}

```
-------------------

## What is the average daily activity pattern?



## Imputing missing values



## Are there differences in activity patterns between weekdays and weekends?
